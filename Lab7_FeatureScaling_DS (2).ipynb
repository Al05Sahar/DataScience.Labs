{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96c9a942",
   "metadata": {},
   "source": [
    "## Lab 7: Feature Scaling\n",
    "\n",
    "Feature scaling is the process of normalising the range of features in a dataset.\n",
    "\n",
    "Real-world datasets often contain features that are varying in degrees of magnitude, range and units. Therefore, in order for machine learning models to interpret these features on the same scale, we have to perform feature scaling.\n",
    "\n",
    "In science, we all know the importance of comparing apples to apples and yet many people, especially beginners, have a tendency to overlook feature scaling as part of the preprocessing steps for machine learning. This has proven to cause models to make inaccurate predictions.\n",
    "\n",
    "In this lab, we will explore why feature scaling is important, the difference between normalisation and standardisation as well as how feature scaling affects model accuracy. More specifically, we will explore the applications of 3 different types of scalers in the Scikit-learn library:\n",
    "\n",
    "1. MixMaxScaler\n",
    "2. StandardScaler\n",
    "3. RobustScaler\n",
    "\n",
    "For the purpose of this tutorial, we will use one of the toy datasets in the Scikit-learn library, the Boston house prices dataset. The details of the data are:\n",
    "\n",
    "Input features in order:\n",
    "1. CRIM: per capita crime rate by town\n",
    "2. ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "3. INDUS: proportion of non-retail business acres per town\n",
    "4. CHAS: Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n",
    "5. NOX: nitric oxides concentration (parts per 10 million) [parts/10M]\n",
    "6. RM: average number of rooms per dwelling\n",
    "7. AGE: proportion of owner-occupied units built prior to 1940\n",
    "8. DIS: weighted distances to five Boston employment centres\n",
    "9. RAD: index of accessibility to radial highways\n",
    "10. TAX: full-value property-tax rate per $10,000 [$/10k]\n",
    "11. PTRATIO: pupil-teacher ratio by town\n",
    "12. B: The result of the equation B=1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "13. LSTAT: % lower status of the population\n",
    "\n",
    "Output variable:\n",
    "1. MEDV: Median value of owner-occupied homes in $1000's [k$]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83cab4d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296.0   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242.0   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242.0   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222.0   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "houses = pd.read_csv('boston.csv')\n",
    "houses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "334b7cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRIM       0\n",
       "ZN         0\n",
       "INDUS      0\n",
       "CHAS       0\n",
       "NOX        0\n",
       "RM         0\n",
       "AGE        0\n",
       "DIS        0\n",
       "RAD        0\n",
       "TAX        0\n",
       "PTRATIO    0\n",
       "B          0\n",
       "LSTAT      0\n",
       "MEDV       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "houses.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a569fa5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (506, 13)\n",
      "Y shape:  (506,)\n"
     ]
    }
   ],
   "source": [
    "# Get predictor and target variables\n",
    "X = houses.drop('MEDV', axis = 1)\n",
    "Y = houses['MEDV']\n",
    "\n",
    "# X, Y shape\n",
    "print(\"X shape: \", X.shape)\n",
    "print(\"Y shape: \", Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00062e53",
   "metadata": {},
   "source": [
    "#### Different Types of Feature Scaling\n",
    "\n",
    "Before we examine the effects of feature scaling, let us first go over some theories behind normalisation and standardisation.\n",
    "\n",
    "1. Normalisation\n",
    "Normalisation, also known as min-max scaling, is a scaling technique whereby the values in a column are shifted so that they are bounded between a fixed range of 0 and 1.\n",
    "\n",
    "X_new = (X - X_min) / (X_max - X_min)\n",
    "\n",
    "MinMaxScaler is the Scikit-learn function for normalisation.\n",
    "\n",
    "2. Standardisation\n",
    "On the other hand, standardisation or Z-score normalisation is another scaling technique whereby the values in a column are rescaled so that they demonstrate the properties of a standard Gaussian distribution, that is mean = 0 and variance = 1. It has average almost zero or close to zero. standardization is commonly used as it shows better results, but in image processing the normalizationndo better.\n",
    "\n",
    "X_new = (X - mean) / std\n",
    "\n",
    "StandardScaler is the Scikit-learn function for standardisation.\n",
    "\n",
    "Unlike StandardScaler, RobustScaler scales features using statistics that are robust to outliers. More specifically, this scaler removes the median and scales the data according to the quantile range or by default, the interquartile range, thus making it less susceptible to outliers.\n",
    "\n",
    "3. Normalisation vs standardisation\n",
    "The choice between normalisation or standardisation comes down to the application.\n",
    "\n",
    "Standardisation is generally preferred over normalisation in most machine learning context as it is especially important in order to compare the similarities between features based on certain distance measures. This is most prominent in Principal Component Analysis (PCA) where we are interested in the components that maximise the variance.\n",
    "\n",
    "Normalisation, on the other hand, also offers many practical applications particularly in computer vision and image processing where pixel intensities have to be normalised to fit within a the RGB colour range between 0 and 255. Furthermore, neural network algorithms typically require data to be normalised to a 0-1 scale before model training.\n",
    "\n",
    "At the end of the day, there is no definitive answer as to whether you should normalise or standardise your data. One can always apply both techniques and compare the model performance for the best results.\n",
    "\n",
    "Now that we have a theoretical understanding of feature scaling, let's see how they work in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4bfb84c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate MinMaxScaler, StandardScaler and RobustScaler\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "norm = MinMaxScaler()\n",
    "standard = StandardScaler()\n",
    "robust = RobustScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32e34d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMaxScaler\n",
    "normalised_features = norm.fit_transform(X) #this return to numpy array instead of df\n",
    "normalised_df = pd.DataFrame(normalised_features, index = X.index, columns = X.columns) #this retrun back to df\n",
    "\n",
    "# StandardScaler\n",
    "standardised_features = standard.fit_transform(X)\n",
    "standardised_df = pd.DataFrame(standardised_features, index = X.index, columns = X.columns)\n",
    "\n",
    "# RobustScaler\n",
    "robust_features = robust.fit_transform(X)\n",
    "robust_df = pd.DataFrame(robust_features, index = X.index, columns = X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b91532",
   "metadata": {},
   "source": [
    "Now that we have the features scaled with the three types of feature scaling, let us now check the impact of this process with a concrete example using the Boston house prices dataset.\n",
    "\n",
    "We will use KNN algorithm for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73deebca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.399660564380535\n",
      "5.445455130560473\n",
      "4.283795660761364\n",
      "4.442559717336984\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "knn = KNeighborsRegressor()\n",
    "\n",
    "# We will test KNN with the the original data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3)\n",
    "knn.fit(X_train, Y_train)\n",
    "pred = knn.predict(X_test)\n",
    "print(np.sqrt(mean_squared_error(Y_test, pred)))\n",
    "\n",
    "\n",
    "# We will test KNN with the normalised_df\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(normalised_df, Y, test_size = 0.3)\n",
    "knn.fit(X_train, Y_train)\n",
    "pred = knn.predict(X_test)\n",
    "print(np.sqrt(mean_squared_error(Y_test, pred)))\n",
    "\n",
    "\n",
    "# We will test KNN with the standardised_df\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(standardised_df, Y, test_size = 0.3)\n",
    "knn.fit(X_train, Y_train)\n",
    "pred = knn.predict(X_test)\n",
    "print(np.sqrt(mean_squared_error(Y_test, pred)))\n",
    "\n",
    "\n",
    "# We will test KNN with the robust_df\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(robust_df, Y, test_size = 0.3)\n",
    "knn.fit(X_train, Y_train)\n",
    "pred = knn.predict(X_test)\n",
    "print(np.sqrt(mean_squared_error(Y_test, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72bd1035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to test different algorithms with the same dataset before and after feature scaling. What do you notice?\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3959f8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.85017579521584\n",
      "6.415316301949848\n",
      "5.528862356295355\n",
      "6.100958048509972\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "svr = SVR()\n",
    "\n",
    "# We will test SVR with the the original data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3)\n",
    "svr.fit(X_train, Y_train)\n",
    "pred = svr.predict(X_test)\n",
    "print(np.sqrt(mean_squared_error(Y_test, pred)))\n",
    "\n",
    "\n",
    "# We will test KNN with the normalised_df\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(normalised_df, Y, test_size = 0.3)\n",
    "svr.fit(X_train, Y_train)\n",
    "pred = svr.predict(X_test)\n",
    "print(np.sqrt(mean_squared_error(Y_test, pred)))\n",
    "\n",
    "\n",
    "# We will test KNN with the standardised_df\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(standardised_df, Y, test_size = 0.3)\n",
    "svr.fit(X_train, Y_train)\n",
    "pred = svr.predict(X_test)\n",
    "print(np.sqrt(mean_squared_error(Y_test, pred)))\n",
    "\n",
    "\n",
    "# We will test KNN with the robust_df\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(robust_df, Y, test_size = 0.3)\n",
    "svr.fit(X_train, Y_train)\n",
    "pred = svr.predict(X_test)\n",
    "print(np.sqrt(mean_squared_error(Y_test, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7a8b0216-ddba-4172-8656-e8f01d3bb3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4719837344998576\n",
      "4.355433690412351\n",
      "3.7258873898851617\n",
      "4.8152512641441785\n"
     ]
    }
   ],
   "source": [
    "dtRegressor = DecisionTreeRegressor()\n",
    "\n",
    "# We will test DecisionTreeRegressor with the the original data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3)\n",
    "dtRegressor.fit(X_train, Y_train)\n",
    "pred = dtRegressor.predict(X_test)\n",
    "print(np.sqrt(mean_squared_error(Y_test, pred)))\n",
    "\n",
    "\n",
    "# We will test KNN with the normalised_df\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(normalised_df, Y, test_size = 0.3)\n",
    "dtRegressor.fit(X_train, Y_train)\n",
    "pred = dtRegressor.predict(X_test)\n",
    "print(np.sqrt(mean_squared_error(Y_test, pred)))\n",
    "\n",
    "\n",
    "# We will test KNN with the standardised_df\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(standardised_df, Y, test_size = 0.3)\n",
    "dtRegressor.fit(X_train, Y_train)\n",
    "pred = dtRegressor.predict(X_test)\n",
    "print(np.sqrt(mean_squared_error(Y_test, pred)))\n",
    "\n",
    "\n",
    "# We will test KNN with the robust_df\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(robust_df, Y, test_size = 0.3)\n",
    "dtRegressor.fit(X_train, Y_train)\n",
    "pred = dtRegressor.predict(X_test)\n",
    "print(np.sqrt(mean_squared_error(Y_test, pred)))\n",
    "\n",
    "#tree algorithm all features are treated equally, that is why normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01649dbb-af19-4eac-a1d5-2eda614676e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
